services:
  dagster:
    build:
      context: .
      dockerfile: src/docker/Dockerfile.dagster
    ports:
      - "3000:3000"
      - "4000:4000"  # gRPC server port
    volumes:
      - .:/app
      - dagster_data:/data
      - dagster_home:/data/dagster_home  # Use named volume for dagster_home
      - dagster_tmp:/tmp/dagster  # Use named volume for temp directory
      - model_storage:/data/models  # Add volume for model storage
    environment:
      - DAGSTER_HOME=/data/dagster_home
      - PYTHONPATH=/app
      - DAGSTER_GRPC_HOST=0.0.0.0
      - DAGSTER_GRPC_PORT=4000
      - OLLAMA_HOST=http://ollama
      - OLLAMA_PORT=11434
      - MODELS_DIR=/data/models
    command: |
      bash -c "
        mkdir -p /data/dagster_home &&
        mkdir -p /data/models/cache &&
        cp /app/src/config/workspace.yaml /data/dagster_home/ &&
        cp /app/src/config/dagster.yaml /data/dagster_home/ &&
        dagster dev -h 0.0.0.0 -p 3000 --grpc-host 0.0.0.0 --grpc-port 4000
      "

  streamlit:
    build:
      context: .
      dockerfile: src/docker/Dockerfile.streamlit
    ports:
      - "8501:8501"
    volumes:
      - .:/app
      - dagster_data:/data
      - dagster_home:/data/dagster_home  # Share dagster_home volume
      - dagster_tmp:/tmp/dagster  # Use same named volume for temp directory
      - model_storage:/data/models  # Share model storage volume
    environment:
      - DAGSTER_HOME=/data/dagster_home
      - DAGSTER_HOST=dagster
      - DAGSTER_PORT=3000
      - PYTHONPATH=/app
      - DAGSTER_GRPC_HOST=dagster
      - DAGSTER_GRPC_PORT=4000
      - OLLAMA_HOST=http://ollama
      - OLLAMA_PORT=11434
      - MODELS_DIR=/data/models
    depends_on:
      - dagster
      - ollama

  ollama:
    image: ollama/ollama:latest
    platform: linux/arm64  # Specify ARM64 platform for M1/M2 Macs
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    deploy:
      resources:
        limits:
          memory: 16G
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
      - OLLAMA_MODELS=/root/.ollama/models
      - OLLAMA_USE_MLX=true  # Enable MLX acceleration
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  test:
    build:
      context: .
      dockerfile: src/docker/Dockerfile.test
    volumes:
      - .:/app
    environment:
      - PYTHONPATH=/app
      - OLLAMA_HOST=http://ollama
      - OLLAMA_PORT=11434
    depends_on:
      ollama:
        condition: service_healthy
    profiles:
      - test

volumes:
  dagster_data:
  dagster_home:  # Define the named volume for dagster_home
  dagster_tmp:  # Define the named volume for temp directory
  model_storage:  # Define the volume for model storage
  ollama_models: 